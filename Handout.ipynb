{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Training on the DTM package: Session 1</center></h1>\n",
    "<div style=\"text-align: center\">Adnane Ez-zizi, 04 May 2020</div> \n",
    "<img src=\"./Figures/logo.png\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- - -\n",
    "\n",
    "##### 0. <a href=#0>Preliminary steps</a>\n",
    "##### I. <a href=#I>Introduction</a>\n",
    "##### II. <a href=#II>The naive discriminative learning model</a>\n",
    "##### III. <a href=#III>Exercises</a>\n",
    "##### IV. <a href=#III>Practical application</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 0. Preliminary things to do <a ID=\"0\"></a> \n",
    "\n",
    "- - -\n",
    "\n",
    "- Install [Jupyter Notebook](https://jupyter.readthedocs.io/en/latest/install.html) \n",
    "- Clone the [Github repo](https://github.com/Adnane017/DTM_training) for the training session\n",
    "- For more information about the DTM package, visit: https://github.com/Adnane017/Deep_text_modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction <a ID=\"I\"></a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples of classification problems on language data\n",
    "\n",
    "- - -\n",
    "\n",
    "Some examples of classification problems on text data from both machine learning and behavioural perspectives:\n",
    "\n",
    "- **Language research:** \n",
    " \n",
    " \n",
    "     - English articles \n",
    "     - English tense\n",
    "     - Russian aspect\n",
    "\n",
    "\n",
    "- **Machine learning (NLP):** \n",
    "\n",
    "\n",
    "     - Sentiment analysis of tweets or customers’ reviews \n",
    "     - Spam filter and email classification\n",
    "     - Plagiarism detection  \n",
    "\n",
    "\n",
    "- **Question:** Any other examples from your work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why are both the machine learning and plausible behavioural approaches interesting? \n",
    "\n",
    "- - -\n",
    "\n",
    "- Why each camp needs to consider the other camp’s approach?  \n",
    "\n",
    "\n",
    "    - Powerful machine learning models (e.g. deep learning) can provide an indication \n",
    "      about the maximum amount of information that can be extracted from the data. \n",
    "    \n",
    "    - Deep learning models are often seen as black boxes. Behaviouraly plausible models do \n",
    "      not suffer as much from this problem.\n",
    "    \n",
    "    - Behaviourally plausible model do not necessarily perform badly. People outperform the \n",
    "      most powerful engineering models in some basic tasks (e.g. )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Deep Text Modelling (DTM) package \n",
    "\n",
    "- - -\n",
    "\n",
    "A python package for processing and modelling text data designed (mainly) for language researchers. <br>\n",
    "Two main objectives behind the creation of the package:\n",
    "\n",
    "1) Make life easier for researchers who want to model language learning. <br>\n",
    "\n",
    "\n",
    "2) Bridge the gap between the machine learning and behavioural worlds by proposing a unifying framework <br> \n",
    "   that both types of users can work with. The framework also allows to easily compare behaviourally <br>            plausible and deep learning models for different types of language data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Main features of DTM\n",
    "\n",
    "- - -\n",
    "\n",
    "- Offers a consistent and easy-to-use code syntax to model language data from small and large corpora.\n",
    "\n",
    "\n",
    "- All the “ugly” stuffs to run Keras algorithms on large corpora are taken care of in the background for the user.\n",
    "\n",
    "\n",
    "- Provides useful tools to speed up the pre-processing and evaluation steps necessary before or after the modelling.\n",
    "\n",
    "\n",
    "- Can train and tune pre-trained and task-specific embeddings in addition to classical one-hot encodings.\n",
    "\n",
    "\n",
    "- For now DTM can work with binary and multiclass classification problems, but the plan is to offer <br>\n",
    "  support for all possibe type of classification problems. \n",
    "\n",
    "\n",
    "- The package comes with multiple examples to illustrate how its models work (more examples will be added in the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. The naive discriminative learning model <a ID=\"II\"></a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The associative learning framework: Pavlov's experiment\n",
    "\n",
    "- - -\n",
    "\n",
    "<img src=\"./Figures/pavlov.png\" width=\"450\"/><center><br>Image source: <a href=\"https://mariyamulwan.wordpress.com/2014/03/02/classical-conditioning-in-behavioural-learning-theory/\">https://mariyamulwan.wordpress.com/2014/03/02/classical-conditioning-in-behavioural-learning-theory</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The associative learning framework for language learning\n",
    "\n",
    "- - -\n",
    "\n",
    "- We will use a different terminology than what is used in Psychology: \n",
    "\n",
    "   - The stimulus of interest that we want to predict is called an outcome (e.g. food). Multiple outcomes are          possible.\n",
    "   - The stimuli that predict the occurence of an outcome are called cues (e.g. sound of a bell)\n",
    "   - A learning event is one experience of the co-occurrence between the cues and outcomes\n",
    "   \n",
    "   \n",
    "- In the same way that the dog in Pavlov’s experiment learned to associate the sound of the bell with food, we can <br> form associations bewteen the language stimuli that we are exposed to.\n",
    "\n",
    "\n",
    "- **Example:** Learn to predict the grammatical number of nouns based on context. The cues could be all the words that <br> surround the noun and the outcomes are whether the noun is singular or plural. Each noun in a sentence would form <br> a seperate event   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Rescorla-Wagner model\n",
    "\n",
    "- - -\n",
    "\n",
    "- The naive discriminative learning model is an adaptation of the Rescorla-Wagner model (R-W; Rescorla and Wagner, 1972) for <br> language learning and processing. R-W describes computationally how the associations between cues and outcomes are acquired. \n",
    "\n",
    "\n",
    "- After encountering each event, the learner updates the association weight between a cue ($i$) and an outcome $j$, depending on <br> whether they appear or not in the event:\n",
    "\n",
    "$$ w^{t+1}_{ij} = w^{t}_{ij} + \\Delta w^{t}_{ij} $$\n",
    "\n",
    "<br>\n",
    "\n",
    "- \\begin{equation}\n",
    "\\Delta w^{t}_{ij} =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t0  & \\mbox{if cue $i$ is present} \\\\\n",
    "        \\gamma(1 - \\sum_{i'}w^{t}_{i'j})  & \\mbox{if cue $i$ and outcome $j$ are present} \\\\\n",
    "\t\t\\gamma(0 - \\sum_{i'}w^{t}_{i'j}) & \\mbox{if cue $i$ is present and outcome $j$ is absent} \n",
    "\t\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "- $t$: current trial\n",
    "- $\\gamma$: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interpretation of the association weights\n",
    "\n",
    "- - -\n",
    "\n",
    "- An association weight measures the tendency of an outcome to be triggered by the presence of a cue. <br><br>\n",
    "\n",
    "- In the grammatical number example, it reflects the tendency of the singular or plural form to occur in the presence of a certain word. <br><br>\n",
    "\n",
    "- A higher positive association weight value for a particular form corresponds to a higher likelihood of occurrence of that form. <br><br>\n",
    "\n",
    "- A lower negative value corresponds to a higher likelihood of non-occurrence of that form. <br><br> \n",
    "\n",
    "- Values close to zero mean low chances of observing the form. <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating choices from the model: Procedure\n",
    "\n",
    "---\n",
    "\n",
    "To generate an outcome choice given a certain set of cues: \n",
    "\n",
    "1) We calculate the activation of each outcome by summing the association weights between the outcome and each of the cues.\n",
    "\n",
    "2) We convert the computed activations into choice probability using the [softmax rule](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "\n",
    "3) The predicted choice from the model is then the outcome that has the highest softmax probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating choices from the model: An example\n",
    "\n",
    "---\n",
    "\n",
    "Back to our grammatical number example, if we have the following sentence: \"the players gathered on the pitch\", then the activations of the singular (s) and plural (p) are as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "a(s)= w(the, s) + w(gathered, s) + w(on, s) + w(pitch, s)\\\\\n",
    "a(p)= w(the, p) + w(gathered, p) + w(on, p) + w(pitch, p)\n",
    "\\end{equation}\n",
    "\n",
    "The softmax probabilities are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "prob(s)= \\frac{e^{a(s)}}{e^{a(s)} + e^{a(p)}}\\\\\n",
    "prob(p)= \\frac{e^{a(p)}}{e^{a(s)} + e^{a(p)}}\n",
    "\\end{equation}\n",
    "\n",
    "If prob(s)>prob(p), the model would predict the singular form, otherwise it would predict the plural form.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Exercises <a ID=\"III\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1:\n",
    "\n",
    "- - -\n",
    "\n",
    "This is an example that you can (and should) try with your pocket calculator and paper and pencil, to fully understand how the association weights are updated. The example is presented in pdf-file `Handmade_Tomatoes.pdf`. You can also later read the following three blog posts for a more detailed presentation of the NDL model:\n",
    "\n",
    "- [https://outofourminds.bham.ac.uk/blog/8](https://outofourminds.bham.ac.uk/blog/8)\n",
    "- [https://outofourminds.bham.ac.uk/blog/9](https://outofourminds.bham.ac.uk/blog/9)\n",
    "- [https://outofourminds.bham.ac.uk/blog/10](https://outofourminds.bham.ac.uk/blog/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "- - -\n",
    "\n",
    "This is the same example like the one above, but this time you will be using DTM to learn the weights.\n",
    "\n",
    "To run this code, please open jupyter notebook `Python_Tomatoes.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 4. Practical application\n",
    "\n",
    "- - -\n",
    "\n",
    "In our practical application, we will use the popular IMDB dataset where we will try to predict the sentiment of a movie review (positive or negative). The code is in the jupyter notebook `Sentiment_analysis.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**THE END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
