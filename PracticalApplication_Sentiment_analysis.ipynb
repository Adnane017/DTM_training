{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical application: Sentiment analysis of movie reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "##### I. <a href=#I>Preliminary steps</a>\n",
    "##### II. <a href=#II>Prepare the data</a>\n",
    "##### III. <a href=#V>Naive discriminative learning model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Preliminary steps <a ID=\"I\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and set up the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Import necessary packages\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.optimizers import Adam, Nadam, RMSprop, SGD\n",
    "from keras.activations import relu, elu\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "\n",
    "### Set working directory\n",
    "#WD = 'F:/Adnane/Teaching/Tutorials_ooominds/DTM_tutorial/'\n",
    "WD = '/media/adnane/HDD drive/Adnane/Teaching/Tutorials_ooominds/DTM_tutorial/'\n",
    "os.chdir(WD)\n",
    "\n",
    "### Import local packages\n",
    "import deep_text_modelling.preprocessing as pr\n",
    "import deep_text_modelling.modelling as md\n",
    "import deep_text_modelling.evaluation as ev\n",
    "\n",
    "# Display option for dataframes and matplotlib\n",
    "pd.set_option('display.max_colwidth', 100) # Max width of columns when displaying a dataframe\n",
    "PREVIOUS_MAX_ROWS = pd.options.display.max_rows\n",
    "pd.options.display.max_rows = 20\n",
    "warnings.filterwarnings('ignore') # Hide warnings\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_FULL_CSV = WD + \"Data/IMDB_full.csv\"\n",
    "IMDB_TRAIN_CSV = WD + \"Data/IMDB_train.csv\"\n",
    "IMDB_VALID_CSV = WD + \"Data/IMDB_valid.csv\"\n",
    "IMDB_TEST_CSV = WD + \"Data/IMDB_test.csv\"\n",
    "CUE_INDEX = WD + \"Data/Cue_index.csv\"\n",
    "OUTCOME_INDEX = WD + \"Data/Outcome_index.csv\"\n",
    "DATA_DIR = WD + 'Data/'\n",
    "GLOVE_PATH = os.path.join(DATA_DIR, 'glove.6B.100d.txt')\n",
    "WORD2VEC_PATH = os.path.join(DATA_DIR, 'GoogleNews-vectors-negative300.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_outcomes = 2 # number of most frequent outcomes to keep \n",
    "N_cues = 2000  # number of most frequent words to keep\n",
    "prop_valid = 1/8 # proportion of validation data\n",
    "prop_test = 1/8 # proportion of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Prepare the data <a name=\"II\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imdb_full = pd.read_csv(IMDB_FULL_CSV)\n",
    "imdb_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use only 2000 reviews from each category to speed up training\n",
    "N_reviews = 4000\n",
    "ind_select = list(range(int(N_reviews/2))) + list(range(50000-int(N_reviews/2), 50000))\n",
    "imdb_full = imdb_full.iloc[ind_select,]\n",
    "print(f'Number of examples: {len(imdb_full)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed characters\n",
    "ENGLISH = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "#ENGLISH = ENGLISH + ENGLISH.upper()\n",
    "not_allowed_symbols = re.compile(\"[^%s]\" % ENGLISH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lower-case all letters\n",
    "imdb_full['review_cleaned'] = imdb_full['review'].apply(lambda s: s.lower())\n",
    "# Replace special characters with spaces\n",
    "imdb_full['review_cleaned'] = imdb_full['review_cleaned'].apply(lambda s: not_allowed_symbols.sub(\" \", s))\n",
    "# Remove multiple spaces\n",
    "imdb_full['review_cleaned'] = imdb_full['review_cleaned'].apply(lambda s: re.sub('\\s+', ' ', s))\n",
    "imdb_full.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the cues and outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_full['cues'] = imdb_full['review_cleaned'].apply(lambda s: \"_\".join(s.split()))\n",
    "imdb_full['outcomes'] = imdb_full['sentiment']\n",
    "imdb_full.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the 'cues' and 'outcomes' columns\n",
    "imdb_full = imdb_full[['cues', 'outcomes']]\n",
    "imdb_full.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index systems for the cues and outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the files containing the index systems\n",
    "pr.create_index_systems_from_df(data = imdb_full, \n",
    "                                cue_index_path = CUE_INDEX, \n",
    "                                outcome_index_path = OUTCOME_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the cue index system\n",
    "cue_to_index = pr.import_index_system(CUE_INDEX, N_tokens = N_cues)\n",
    "pr.display_dictionary(cue_to_index, start = 0, end = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the outcome index system\n",
    "outcome_to_index = pr.import_index_system(OUTCOME_INDEX)\n",
    "outcome_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the cue dictionary\n",
    "index_to_cue = pr.reverse_dictionary(cue_to_index)\n",
    "# Reverse the outcome dictionary\n",
    "index_to_outcome = pr.reverse_dictionary(outcome_to_index)\n",
    "index_to_outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train, valid and test set files\n",
    "pr.df_train_valid_test_split(data = imdb_full, \n",
    "                             train_data_path = IMDB_TRAIN_CSV, \n",
    "                             valid_data_path = IMDB_VALID_CSV, \n",
    "                             test_data_path = IMDB_TEST_CSV, \n",
    "                             p_valid = prop_valid, \n",
    "                             p_test = prop_test,\n",
    "                             seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train, valid and test sets\n",
    "imdb_train = pd.read_csv(IMDB_TRAIN_CSV, sep=',', na_filter = False)\n",
    "imdb_valid = pd.read_csv(IMDB_VALID_CSV, sep=',', na_filter = False)\n",
    "imdb_test = pd.read_csv(IMDB_VALID_CSV, sep=',', na_filter = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Naive discriminative learning model <a ID=\"V\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple NDL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Build a simple NDL\n",
    "p = {'epochs': 10, # number of iterations on the full set \n",
    "    'lr': 0.001}\n",
    "\n",
    "# Model fitting\n",
    "NDL_history_dict, NDL_model = md.train(model = 'NDL',\n",
    "                                       data_train = imdb_train, \n",
    "                                       data_valid = imdb_valid,  \n",
    "                                       cue_index = cue_to_index, \n",
    "                                       outcome_index = outcome_to_index, \n",
    "                                       num_threads = 16, \n",
    "                                       verbose = 1,\n",
    "                                       params = p,\n",
    "                                       temp_dir = DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate learning curve\n",
    "ev.plot_learning_curve(history_dict = NDL_history_dict, metric = 'accuracy', set = 'train_valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the parameters to find a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter tuning using grid search \n",
    "p = {'lr': [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05], # learning rate (x8)\n",
    "     'epochs': [1, 2, 4, 6], # number of iterations on the full set (x4)\n",
    "    }\n",
    "\n",
    "### Grid search \n",
    "TUNING_PATH = WD + 'Results/grid_search_NDL_imdb.csv'\n",
    "md.grid_search(model = 'NDL',\n",
    "               data_train = imdb_train, \n",
    "               data_valid = imdb_valid, \n",
    "               cue_index = cue_to_index, \n",
    "               outcome_index = outcome_to_index, \n",
    "               params = p, \n",
    "               prop_grid = 0.2, \n",
    "               shuffle_grid = True,\n",
    "               tuning_output_file = TUNING_PATH, \n",
    "               temp_dir = DATA_DIR,\n",
    "               num_threads = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the grid search file to analyse the results \n",
    "gs_results = pd.read_csv(TUNING_PATH, index_col = False)\n",
    "\n",
    "# get the number of parameter combinations that were processed\n",
    "len(gs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe containing the tuning results\n",
    "gs_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the highest result for any metric\n",
    "print(f\"- Highest validation accuracy: {gs_results['val_acc'].max()}\")\n",
    "print(f\"- Highest validation f1-score: {gs_results['f1score'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the best paramaters\n",
    "i_best = gs_results['val_acc'].argmax()\n",
    "gs_results.iloc[i_best, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Hyperparameters to use\n",
    "p = {'epochs': 3, # number of iterations on the full set \n",
    "    'lr': 0.001}\n",
    "\n",
    "# Model fitting\n",
    "NDL_hist, NDL_model = md.train(model = 'NDL',\n",
    "                               data_train = imdb_train, \n",
    "                               data_valid = imdb_valid,  \n",
    "                               cue_index = cue_to_index, \n",
    "                               outcome_index = outcome_to_index, \n",
    "                               num_threads = 16, \n",
    "                               verbose = 1,\n",
    "                               params = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights and training history\n",
    "MODEL_PATH = WD + 'Results/NDL_imdb.h5'\n",
    "HISTORY_PATH = WD + 'Results/NDL_history_dict_imdb'\n",
    "md.export_model(model = NDL_model, path = MODEL_PATH)  # create a HDF5 file \n",
    "md.export_history(history_dict = NDL_hist, path = HISTORY_PATH)\n",
    "del NDL_model, NDL_hist  # delete the existing model and history dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and training history\n",
    "MODEL_PATH = WD + 'Results/NDL_imdb.h5'\n",
    "HISTORY_PATH = WD + 'Results/NDL_history_dict_imdb'\n",
    "NDL_model = md.import_model(MODEL_PATH)\n",
    "NDL_history_dict = md.import_history(path = HISTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test prediction for a single given cue sequence. Model expect input as array of shape (1, N_cues) \n",
    "cue1_seq = 'it_is_the_worst_film_ever' # context from the sentence 'I will meet you tomorrow'\n",
    "outcome1_prob_pred = ev.predict_proba_oneevent_NDL(model = NDL_model, \n",
    "                                                   cue_seq = cue1_seq)\n",
    "print(outcome1_prob_pred) # vector of predicted probabilities\n",
    "print({index_to_outcome[j+1]:outcome1_prob_pred[j] for j in range(len(outcome1_prob_pred))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of neg seems low (the model is quite unconfident), because we trained it on a small dataset (with 3000 examples). One way to increase the confidence of the model is to add a temperature parameter to the softmax function and set it at a low value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test prediction for a single given cue sequence. Model expect input as array of shape (1, N_cues) \n",
    "cue1_seq = 'it_is_the_worst_film_ever'\n",
    "outcome1_prob_pred = ev.predict_proba_oneevent_NDL(model = NDL_model, \n",
    "                                                   cue_seq = cue1_seq,\n",
    "                                                   T = 0.1)\n",
    "print(outcome1_prob_pred) # vector of predicted probabilities\n",
    "print({index_to_outcome[j+1]:outcome1_prob_pred[j] for j in range(len(outcome1_prob_pred))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the model on the test set\n",
    "# True outcomes to compare the predictions to\n",
    "y_test = imdb_test['outcomes'].tolist()\n",
    "\n",
    "# Predicted outcomes\n",
    "y_pred = ev.predict_outcomes_NDL(model = NDL_model, \n",
    "                                 data_test = imdb_test,\n",
    "                                 num_threads = 16,\n",
    "                                 temp_dir = DATA_DIR,\n",
    "                                 remove_temp_dir = False)\n",
    "\n",
    "# Overall test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy per class\n",
    "cmat = confusion_matrix(y_test, y_pred, labels = list(outcome_to_index.keys())) # Confusion matrix\n",
    "cmat_diag = cmat.diagonal()/cmat.sum(axis=1)\n",
    "print({index_to_outcome[j+1]:cmat_diag[j] for j in range(len(cmat_diag))})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
